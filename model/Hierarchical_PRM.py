# Hierarchical_PRM.py
"""
å®Œæ•´çš„åˆ†å±‚é¢„æµ‹è¡¨ç¤ºæ¨¡å‹ï¼Œé›†æˆh-DQNè®ºæ–‡ä¸­çš„Îµ-greedyé€€ç«ç®—æ³•
åŒ…å«åŒå±‚æ¢ç´¢ç­–ç•¥ã€æˆåŠŸç‡è·Ÿè¸ªå’ŒåŠ¨æ€é€€ç«æœºåˆ¶
"""

from typing import List, Optional, Tuple, Dict
import torch
import torch.nn.functional as F
from torch import nn
import os
import math
import numpy as np
from torch.distributions import Categorical, Normal
import random
from collections import defaultdict, deque

# å¯¼å…¥åŸå§‹PRMç»„ä»¶
try:
    from model.PRM import (
        Encoder, ImageDecoder, ActionEncoder, TransformerLayer,
        create_optimized_optimizer, create_lr_scheduler
    )
except ImportError:
    print("Warning: Could not import from model.PRM, using embedded components")


class OptionEncoder(nn.Module):
    """é€‰é¡¹ç¼–ç å™¨ï¼Œå°†é€‰é¡¹IDç¼–ç ä¸ºå‘é‡"""

    def __init__(self, num_options=4, embed_dim=128, output_dim=256):
        super().__init__()
        self.option_embedding = nn.Embedding(num_options, embed_dim)
        self.option_proj = nn.Sequential(
            nn.Linear(embed_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )

    def forward(self, options):
        """
        Args:
            options: (B,) é€‰é¡¹ç´¢å¼•
        Returns:
            option_features: (B, output_dim) é€‰é¡¹ç‰¹å¾
        """
        embedded = self.option_embedding(options.long())
        return self.option_proj(embedded)


class HighLevelPolicy(nn.Module):
    """é«˜å±‚ç­–ç•¥ç½‘ç»œ - é€‰æ‹©é€‰é¡¹ (Meta-controller)"""

    def __init__(self, state_dim=256, num_options=4, hidden_dim=512):
        super().__init__()
        self.num_options = num_options

        self.policy_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, num_options)
        )

    def forward(self, state_features):
        """
        Args:
            state_features: (B, state_dim) çŠ¶æ€ç‰¹å¾
        Returns:
            option_logits: (B, num_options) é€‰é¡¹logits
        """
        return self.policy_net(state_features)

    def sample_option(self, state_features):
        """é‡‡æ ·é€‰é¡¹"""
        logits = self.forward(state_features)
        dist = Categorical(logits=logits)
        option = dist.sample()
        log_prob = dist.log_prob(option)
        return option, log_prob, dist.entropy()


class OptionPolicy(nn.Module):
    """é€‰é¡¹ç­–ç•¥ç½‘ç»œ - åœ¨ç»™å®šé€‰é¡¹ä¸‹é€‰æ‹©åŠ¨ä½œ (Controller)"""

    def __init__(self, state_dim=256, option_dim=256, action_dim=19, hidden_dim=512):
        super().__init__()
        self.action_dim = action_dim

        # èåˆçŠ¶æ€å’Œé€‰é¡¹ä¿¡æ¯
        self.fusion_net = nn.Sequential(
            nn.Linear(state_dim + option_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )

        # åŠ¨ä½œæ¦‚ç‡è¾“å‡º
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, action_dim)
        )

    def forward(self, state_features, option_features):
        """
        Args:
            state_features: (B, state_dim) çŠ¶æ€ç‰¹å¾
            option_features: (B, option_dim) é€‰é¡¹ç‰¹å¾
        Returns:
            action_logits: (B, action_dim) åŠ¨ä½œlogits
        """
        fused = torch.cat([state_features, option_features], dim=-1)
        hidden = self.fusion_net(fused)
        action_logits = self.action_head(hidden)
        return action_logits

    def sample_action(self, state_features, option_features):
        """é‡‡æ ·åŠ¨ä½œ"""
        logits = self.forward(state_features, option_features)
        dist = Categorical(logits=logits)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action, log_prob, dist.entropy()


class TerminationNetwork(nn.Module):
    """ç»ˆæ­¢ç½‘ç»œ - å†³å®šæ˜¯å¦ç»ˆæ­¢å½“å‰é€‰é¡¹"""

    def __init__(self, state_dim=256, option_dim=256, hidden_dim=512):
        super().__init__()

        self.termination_net = nn.Sequential(
            nn.Linear(state_dim + option_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()  # ç»ˆæ­¢æ¦‚ç‡ [0, 1]
        )

    def forward(self, state_features, option_features):
        """
        Args:
            state_features: (B, state_dim) çŠ¶æ€ç‰¹å¾
            option_features: (B, option_dim) é€‰é¡¹ç‰¹å¾
        Returns:
            termination_prob: (B, 1) ç»ˆæ­¢æ¦‚ç‡
        """
        fused = torch.cat([state_features, option_features], dim=-1)
        return self.termination_net(fused)

    def should_terminate(self, state_features, option_features):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»ˆæ­¢"""
        prob = self.forward(state_features, option_features)
        return torch.bernoulli(prob)


class ValueNetwork(nn.Module):
    """ä»·å€¼ç½‘ç»œ - ä¼°è®¡çŠ¶æ€ä»·å€¼"""

    def __init__(self, state_dim=256, hidden_dim=512):
        super().__init__()

        self.value_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, state_features):
        """
        Args:
            state_features: (B, state_dim) çŠ¶æ€ç‰¹å¾
        Returns:
            values: (B, 1) çŠ¶æ€ä»·å€¼
        """
        return self.value_net(state_features)


class HierarchicalPRM(nn.Module):
    """åˆ†å±‚é¢„æµ‹è¡¨ç¤ºæ¨¡å‹"""

    def __init__(
            self,
            # PRMé…ç½®
            img_in_channels: int = 3,
            img_out_channels: int = 3,
            encoder_layers: List[int] = [2, 3, 4, 3],
            decoder_layers: List[int] = [2, 2, 2, 1],
            base_channels: int = 64,
            latent_dim: int = 256,

            # åˆ†å±‚RLé…ç½®
            num_options: int = 4,  # é€‰é¡¹æ•°é‡
            action_dim: int = 19,  # åŠ¨ä½œç»´åº¦ (0-18)
            hidden_dim: int = 512,

            # Transformeré…ç½®
            num_attention_heads: int = 8,
            transformer_layers: int = 3,

            # è®­ç»ƒé…ç½®
            use_skip_connections: bool = True,
            loss_weights: Dict[str, float] = None,
            dropout: float = 0.1
    ):
        super().__init__()

        if loss_weights is None:
            loss_weights = {
                'img': 1.0,  # å›¾åƒé‡å»ºæŸå¤±
                'done': 1.0,  # doneé¢„æµ‹æŸå¤±
                'vector': 1.0,  # ç‰¹å¾ä¸€è‡´æ€§æŸå¤±
                'policy': 1.0,  # ç­–ç•¥æŸå¤±
                'value': 0.5,  # ä»·å€¼æŸå¤±
                'entropy': 0.01,  # ç†µæ­£åˆ™åŒ–
                'termination': 0.1  # ç»ˆæ­¢æŸå¤±
            }

        self.num_options = num_options
        self.action_dim = action_dim
        self.latent_dim = latent_dim
        self.loss_weights = loss_weights

        # === PRM ç»„ä»¶ ===
        self.encoder = Encoder(img_in_channels, encoder_layers, base_channels)
        self.image_decoder = ImageDecoder(
            latent_dim, img_out_channels, decoder_layers,
            base_channels, use_skip_connections
        )

        # === åˆ†å±‚RLç»„ä»¶ ===
        self.option_encoder = OptionEncoder(num_options, 128, latent_dim)
        self.high_level_policy = HighLevelPolicy(latent_dim, num_options, hidden_dim)
        self.option_policy = OptionPolicy(latent_dim, latent_dim, action_dim, hidden_dim)
        self.termination_network = TerminationNetwork(latent_dim, latent_dim, hidden_dim)
        self.value_network = ValueNetwork(latent_dim, hidden_dim)

        # === Doneåˆ†ç±»å™¨ ===
        self.done_classifier = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )

        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)

    def _init_weights(self, m):
        """åˆå§‹åŒ–æƒé‡"""
        if isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Embedding):
            nn.init.normal_(m.weight, mean=0, std=0.02)
        elif isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
            nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, (nn.BatchNorm2d, nn.LayerNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)

    def encode_state(self, images):
        """ç¼–ç çŠ¶æ€"""
        state_features, skip_features = self.encoder(images)
        return state_features, skip_features

    def predict_next_frame(self, state_features, skip_features=None):
        """é¢„æµ‹ä¸‹ä¸€å¸§å›¾åƒ"""
        if skip_features is not None:
            predicted_image = self.image_decoder(state_features, skip_features)
        else:
            predicted_image = self.image_decoder(state_features)
        return predicted_image

    def select_option(self, state_features):
        """é€‰æ‹©é€‰é¡¹"""
        return self.high_level_policy.sample_option(state_features)

    def select_action(self, state_features, option):
        """æ ¹æ®é€‰é¡¹é€‰æ‹©åŠ¨ä½œ"""
        option_features = self.option_encoder(option)
        return self.option_policy.sample_action(state_features, option_features)

    def should_terminate_option(self, state_features, option):
        """åˆ¤æ–­æ˜¯å¦ç»ˆæ­¢é€‰é¡¹"""
        option_features = self.option_encoder(option)
        return self.termination_network.should_terminate(state_features, option_features)

    def get_state_value(self, state_features):
        """è·å–çŠ¶æ€ä»·å€¼"""
        return self.value_network(state_features)

    def forward(self, images, options=None, actions=None, return_all=False):
        """
        å‰å‘ä¼ æ’­
        Args:
            images: (B, 3, H, W) è¾“å…¥å›¾åƒ
            options: (B,) é€‰é¡¹ (å¯é€‰)
            actions: (B,) åŠ¨ä½œ (å¯é€‰)
            return_all: æ˜¯å¦è¿”å›æ‰€æœ‰ä¸­é—´ç»“æœ
        """
        batch_size = images.size(0)

        # ç¼–ç çŠ¶æ€
        state_features, skip_features = self.encode_state(images)

        # é¢„æµ‹ä¸‹ä¸€å¸§
        predicted_images = self.predict_next_frame(state_features, skip_features)

        # Doneé¢„æµ‹
        done_predictions = self.done_classifier(state_features)

        # çŠ¶æ€ä»·å€¼
        state_values = self.get_state_value(state_features)

        results = {
            'state_features': state_features,
            'predicted_images': predicted_images,
            'done_predictions': done_predictions,
            'state_values': state_values
        }

        # å¦‚æœæä¾›äº†é€‰é¡¹ï¼Œè®¡ç®—ç­–ç•¥ç›¸å…³è¾“å‡º
        if options is not None:
            option_features = self.option_encoder(options)

            # åŠ¨ä½œæ¦‚ç‡
            action_logits = self.option_policy(state_features, option_features)
            results['action_logits'] = action_logits

            # ç»ˆæ­¢æ¦‚ç‡
            termination_probs = self.termination_network(state_features, option_features)
            results['termination_probs'] = termination_probs

        # é«˜å±‚ç­–ç•¥ - é€‰é¡¹é€‰æ‹©
        option_logits = self.high_level_policy(state_features)
        results['option_logits'] = option_logits

        if return_all:
            results['skip_features'] = skip_features

        return results

    def compute_hierarchical_loss(self,
                                  batch_data: Dict[str, torch.Tensor],
                                  predictions: Dict[str, torch.Tensor],
                                  target_data: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        è®¡ç®—åˆ†å±‚å­¦ä¹ æŸå¤±
        """
        losses = {}

        # 1. å›¾åƒé‡å»ºæŸå¤±
        if 'predicted_images' in predictions and 'target_images' in target_data:
            losses['img'] = F.mse_loss(
                predictions['predicted_images'],
                target_data['target_images']
            )

        # 2. Doneé¢„æµ‹æŸå¤±
        if 'done_predictions' in predictions and 'target_done' in target_data:
            target_done_float = target_data['target_done'].float().view(-1, 1)
            losses['done'] = F.binary_cross_entropy(
                predictions['done_predictions'],
                target_done_float
            )

        # 3. ç‰¹å¾ä¸€è‡´æ€§æŸå¤±
        if 'state_features' in predictions and 'target_features' in target_data:
            pred_norm = F.normalize(predictions['state_features'], p=2, dim=1)
            target_norm = F.normalize(target_data['target_features'], p=2, dim=1)
            cosine_sim = (pred_norm * target_norm).sum(dim=1)
            losses['vector'] = (1.0 - cosine_sim).mean()

        # 4. ç­–ç•¥æŸå¤± (å¦‚æœæœ‰åŠ¨ä½œæ ‡ç­¾)
        if 'action_logits' in predictions and 'target_actions' in target_data:
            losses['policy'] = F.cross_entropy(
                predictions['action_logits'],
                target_data['target_actions']
            )

        # 5. ä»·å€¼æŸå¤± (å¦‚æœæœ‰ä»·å€¼æ ‡ç­¾)
        if 'state_values' in predictions and 'target_values' in target_data:
            losses['value'] = F.mse_loss(
                predictions['state_values'].squeeze(),
                target_data['target_values']
            )

        # 6. ç†µæ­£åˆ™åŒ–
        if 'option_logits' in predictions:
            option_dist = Categorical(logits=predictions['option_logits'])
            losses['entropy'] = -option_dist.entropy().mean()

        # 7. ç»ˆæ­¢æŸå¤± (å¦‚æœæœ‰ç»ˆæ­¢æ ‡ç­¾)
        if 'termination_probs' in predictions and 'target_termination' in target_data:
            target_term_float = target_data['target_termination'].float().view(-1, 1)
            losses['termination'] = F.binary_cross_entropy(
                predictions['termination_probs'],
                target_term_float
            )

        # è®¡ç®—æ€»æŸå¤±
        total_loss = 0
        loss_dict = {}
        for loss_name, loss_value in losses.items():
            weight = self.loss_weights.get(loss_name, 1.0)
            weighted_loss = weight * loss_value
            total_loss += weighted_loss
            loss_dict[f'loss_{loss_name}'] = loss_value.item()
            loss_dict[f'weighted_loss_{loss_name}'] = weighted_loss.item()

        loss_dict['total_loss'] = total_loss.item()

        # è®¡ç®—ä¸€äº›é¢å¤–æŒ‡æ ‡
        if 'done_predictions' in predictions and 'target_done' in target_data:
            done_pred_binary = (predictions['done_predictions'] > 0.5).float()
            done_accuracy = (done_pred_binary.view(-1) == target_data['target_done'].float()).float().mean()
            loss_dict['done_accuracy'] = done_accuracy.item()

        if 'action_logits' in predictions and 'target_actions' in target_data:
            action_pred = predictions['action_logits'].argmax(dim=1)
            action_accuracy = (action_pred == target_data['target_actions']).float().mean()
            loss_dict['action_accuracy'] = action_accuracy.item()

        return total_loss, loss_dict


# ============================================================================
# ğŸ”¥ Îµ-greedyé€€ç«ç®—æ³•ç»„ä»¶ (h-DQNè®ºæ–‡å®ç°)
# ============================================================================

class EpsilonAnnealingScheduler:
    """
    ğŸ”¥ Îµ-greedyé€€ç«è°ƒåº¦å™¨
    å®ç°h-DQNè®ºæ–‡ä¸­çš„æ¢ç´¢æ¦‚ç‡é€€ç«æœºåˆ¶

    Learning Algorithmä¸­æè¿°çš„é€€ç«ç­–ç•¥ï¼š
    - Meta-controller: Îµ2ä»1é€€ç«åˆ°0.1
    - Controller: Îµ1,gåŸºäºæˆåŠŸç‡åŠ¨æ€è°ƒæ•´ï¼ˆæˆåŠŸç‡>90%æ—¶Îµ1,g=0.1ï¼Œå¦åˆ™é€€ç«ï¼‰
    """

    def __init__(self,
                 initial_epsilon_meta: float = 1.0,  # Meta-controlleråˆå§‹Îµ2
                 final_epsilon_meta: float = 0.1,  # Meta-controlleræœ€ç»ˆÎµ2
                 initial_epsilon_controller: float = 1.0,  # Controlleråˆå§‹Îµ1
                 final_epsilon_controller: float = 0.1,  # Controlleræœ€ç»ˆÎµ1
                 success_threshold: float = 0.9,  # æˆåŠŸç‡é˜ˆå€¼(90%)
                 anneal_steps_meta: int = 50000,  # Meta-controlleré€€ç«æ­¥æ•°
                 anneal_steps_controller: int = 100000):  # Controlleré€€ç«æ­¥æ•°

        self.initial_epsilon_meta = initial_epsilon_meta
        self.final_epsilon_meta = final_epsilon_meta
        self.initial_epsilon_controller = initial_epsilon_controller
        self.final_epsilon_controller = final_epsilon_controller
        self.success_threshold = success_threshold
        self.anneal_steps_meta = anneal_steps_meta
        self.anneal_steps_controller = anneal_steps_controller

        self.current_step = 0

        # ğŸ”¥ è®°å½•æ¯ä¸ªç›®æ ‡(é€‰é¡¹)çš„æˆåŠŸç‡ - h-DQN Algorithm 3
        self.goal_success_rates = defaultdict(float)
        self.goal_attempts = defaultdict(int)
        self.goal_successes = defaultdict(int)

        print(f"ğŸ”¥ h-DQN Îµ-greedyé€€ç«è°ƒåº¦å™¨åˆå§‹åŒ–:")
        print(f"   Meta-controller: Îµ2 {initial_epsilon_meta} â†’ {final_epsilon_meta} ({anneal_steps_meta}æ­¥)")
        print(
            f"   Controller: Îµ1 {initial_epsilon_controller} â†’ {final_epsilon_controller} ({anneal_steps_controller}æ­¥)")
        print(f"   æˆåŠŸç‡é˜ˆå€¼: {success_threshold}")

    def get_epsilon_meta(self) -> float:
        """
        ğŸ”¥ è·å–Meta-controllerçš„å½“å‰Îµ2å€¼
        h-DQNè®ºæ–‡: Îµ2ä»åˆå§‹å€¼1çº¿æ€§é€€ç«
        """
        if self.current_step >= self.anneal_steps_meta:
            return self.final_epsilon_meta

        progress = self.current_step / self.anneal_steps_meta
        epsilon = self.initial_epsilon_meta - (self.initial_epsilon_meta - self.final_epsilon_meta) * progress
        return max(epsilon, self.final_epsilon_meta)

    def get_epsilon_controller(self, goal_id: int) -> float:
        """
        ğŸ”¥ è·å–Controllerå¯¹ç‰¹å®šç›®æ ‡çš„å½“å‰Îµ1,gå€¼
        h-DQNè®ºæ–‡: å¦‚æœæˆåŠŸç‡>90%ï¼Œè®¾ç½®Îµ1,g=0.1ï¼Œå¦åˆ™é€€ç«åˆ°0.1

        Args:
            goal_id: ç›®æ ‡(é€‰é¡¹)ID

        Returns:
            epsilon_1_g: è¯¥ç›®æ ‡çš„æ¢ç´¢æ¦‚ç‡
        """
        success_rate = self.goal_success_rates[goal_id]

        # ğŸ”¥ h-DQNæ ¸å¿ƒé€»è¾‘ï¼šæˆåŠŸç‡>90%æ—¶å¼ºåˆ¶è®¾ç½®Îµ1,g=0.1
        if success_rate > self.success_threshold:
            return self.final_epsilon_controller

        # å¦åˆ™ä½¿ç”¨é€€ç«ç­–ç•¥
        if self.current_step >= self.anneal_steps_controller:
            return self.final_epsilon_controller

        progress = self.current_step / self.anneal_steps_controller
        epsilon = self.initial_epsilon_controller - (
                    self.initial_epsilon_controller - self.final_epsilon_controller) * progress
        return max(epsilon, self.final_epsilon_controller)

    def update_goal_success(self, goal_id: int, success: bool):
        """
        ğŸ”¥ æ›´æ–°ç›®æ ‡æˆåŠŸç‡ç»Ÿè®¡
        h-DQNè®ºæ–‡ä¸­ç”¨äºå†³å®šÎµ1,gçš„å…³é”®æœºåˆ¶

        Args:
            goal_id: ç›®æ ‡ID
            success: æ˜¯å¦æˆåŠŸè¾¾æˆç›®æ ‡
        """
        self.goal_attempts[goal_id] += 1
        if success:
            self.goal_successes[goal_id] += 1

        # è®¡ç®—æˆåŠŸç‡
        if self.goal_attempts[goal_id] > 0:
            self.goal_success_rates[goal_id] = self.goal_successes[goal_id] / self.goal_attempts[goal_id]

    def step(self):
        """æ¨è¿›ä¸€æ­¥ï¼Œæ›´æ–°å½“å‰æ­¥æ•°"""
        self.current_step += 1

    def get_current_stats(self) -> Dict:
        """è·å–å½“å‰ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'current_step': self.current_step,
            'epsilon_meta': self.get_epsilon_meta(),
            'goal_success_rates': dict(self.goal_success_rates),
            'goal_attempts': dict(self.goal_attempts),
            'goal_successes': dict(self.goal_successes)
        }


class GoalSuccessTracker:
    """
    ğŸ”¥ ç›®æ ‡æˆåŠŸè·Ÿè¸ªå™¨
    ç”¨äºåˆ¤æ–­é€‰é¡¹æ‰§è¡Œæ˜¯å¦æˆåŠŸï¼Œæ”¯æŒh-DQNçš„æˆåŠŸç‡è®¡ç®—
    """

    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.goal_windows = defaultdict(lambda: deque(maxlen=window_size))
        self.goal_total_attempts = defaultdict(int)
        self.goal_total_successes = defaultdict(int)

    def add_result(self, goal_id: int, success: bool):
        """æ·»åŠ ç›®æ ‡æ‰§è¡Œç»“æœ"""
        self.goal_windows[goal_id].append(1.0 if success else 0.0)
        self.goal_total_attempts[goal_id] += 1
        if success:
            self.goal_total_successes[goal_id] += 1

    def get_success_rate(self, goal_id: int) -> float:
        """è·å–ç›®æ ‡çš„æ»‘åŠ¨çª—å£æˆåŠŸç‡"""
        if goal_id not in self.goal_windows or len(self.goal_windows[goal_id]) == 0:
            return 0.0
        return sum(self.goal_windows[goal_id]) / len(self.goal_windows[goal_id])

    def get_total_success_rate(self, goal_id: int) -> float:
        """è·å–ç›®æ ‡çš„æ€»ä½“æˆåŠŸç‡"""
        if self.goal_total_attempts[goal_id] == 0:
            return 0.0
        return self.goal_total_successes[goal_id] / self.goal_total_attempts[goal_id]

    def get_all_success_rates(self) -> Dict[int, float]:
        """è·å–æ‰€æœ‰ç›®æ ‡çš„æˆåŠŸç‡"""
        return {goal_id: self.get_success_rate(goal_id) for goal_id in self.goal_windows.keys()}


class HierarchicalAgent:
    """
    ğŸ”¥ åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
    å®ç°h-DQNçš„åŒå±‚Îµ-greedyç­–ç•¥å’Œé€€ç«ç®—æ³•

    æ ¸å¿ƒç‰¹æ€§ï¼š
    - Meta-controller: Îµ2-greedyé€‰é¡¹é€‰æ‹©
    - Controller: Îµ1,g-greedyåŠ¨ä½œé€‰æ‹©ï¼ˆåŸºäºæˆåŠŸç‡ï¼‰
    - åŠ¨æ€é€€ç«æœºåˆ¶
    """

    def __init__(self,
                 model: HierarchicalPRM,
                 epsilon_scheduler: EpsilonAnnealingScheduler,
                 success_tracker: GoalSuccessTracker,
                 num_options: int = 4,
                 max_option_length: int = 8):

        self.model = model
        self.epsilon_scheduler = epsilon_scheduler
        self.success_tracker = success_tracker
        self.num_options = num_options
        self.max_option_length = max_option_length

        # å½“å‰çŠ¶æ€
        self.current_option = None
        self.option_step_count = 0
        self.option_start_state = None
        self.option_start_step = 0

        print(f"ğŸ¤– h-DQNåˆ†å±‚æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
        print(f"   é€‰é¡¹æ•°é‡: {num_options}")
        print(f"   æœ€å¤§é€‰é¡¹é•¿åº¦: {max_option_length}")

    def select_option(self, state_features: torch.Tensor, use_epsilon_greedy: bool = True) -> Tuple[int, bool]:
        """
        ğŸ”¥ Meta-controller: ä½¿ç”¨Îµ2-greedyç­–ç•¥é€‰æ‹©é€‰é¡¹
        h-DQN Algorithm 1 & 2

        Args:
            state_features: çŠ¶æ€ç‰¹å¾
            use_epsilon_greedy: æ˜¯å¦ä½¿ç”¨Îµ-greedyç­–ç•¥

        Returns:
            option_id: é€‰æ‹©çš„é€‰é¡¹ID
            is_random: æ˜¯å¦æ˜¯éšæœºé€‰æ‹©
        """
        if not use_epsilon_greedy:
            # è´ªå©ªç­–ç•¥
            with torch.no_grad():
                option_logits = self.model.high_level_policy(state_features)
                option_id = option_logits.argmax(dim=1).item()
            return option_id, False

        # ğŸ”¥ Îµ2-greedyç­–ç•¥ (h-DQN Meta-controller)
        epsilon_meta = self.epsilon_scheduler.get_epsilon_meta()

        if random.random() < epsilon_meta:
            # éšæœºæ¢ç´¢
            option_id = random.randint(0, self.num_options - 1)
            is_random = True
        else:
            # è´ªå©ªé€‰æ‹©
            with torch.no_grad():
                option_logits = self.model.high_level_policy(state_features)
                option_id = option_logits.argmax(dim=1).item()
            is_random = False

        return option_id, is_random

    def select_action(self,
                      state_features: torch.Tensor,
                      option_id: int,
                      use_epsilon_greedy: bool = True) -> Tuple[int, bool]:
        """
        ğŸ”¥ Controller: ä½¿ç”¨Îµ1,g-greedyç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        h-DQN Algorithm 1 & 2 - åŸºäºç›®æ ‡æˆåŠŸç‡çš„åŠ¨æ€æ¢ç´¢

        Args:
            state_features: çŠ¶æ€ç‰¹å¾
            option_id: å½“å‰é€‰é¡¹ID
            use_epsilon_greedy: æ˜¯å¦ä½¿ç”¨Îµ-greedyç­–ç•¥

        Returns:
            action_id: é€‰æ‹©çš„åŠ¨ä½œID
            is_random: æ˜¯å¦æ˜¯éšæœºé€‰æ‹©
        """
        if not use_epsilon_greedy:
            # è´ªå©ªç­–ç•¥
            with torch.no_grad():
                option_tensor = torch.tensor([option_id], device=state_features.device)
                option_features = self.model.option_encoder(option_tensor)
                action_logits = self.model.option_policy(state_features, option_features)
                action_id = action_logits.argmax(dim=1).item()
            return action_id, False

        # ğŸ”¥ Îµ1,g-greedyç­–ç•¥ (h-DQN Controller)
        epsilon_controller = self.epsilon_scheduler.get_epsilon_controller(option_id)

        if random.random() < epsilon_controller:
            # éšæœºæ¢ç´¢
            action_id = random.randint(0, 18)  # AtariåŠ¨ä½œç©ºé—´0-18
            is_random = True
        else:
            # è´ªå©ªé€‰æ‹©
            with torch.no_grad():
                option_tensor = torch.tensor([option_id], device=state_features.device)
                option_features = self.model.option_encoder(option_tensor)
                action_logits = self.model.option_policy(state_features, option_features)
                action_id = action_logits.argmax(dim=1).item()
            is_random = False

        return action_id, is_random

    def should_terminate_option(self, state_features: torch.Tensor, option_id: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»ˆæ­¢å½“å‰é€‰é¡¹
        ç»“åˆç½‘ç»œé¢„æµ‹å’Œå¼ºåˆ¶ç»ˆæ­¢æ¡ä»¶

        Args:
            state_features: å½“å‰çŠ¶æ€ç‰¹å¾
            option_id: å½“å‰é€‰é¡¹ID

        Returns:
            should_terminate: æ˜¯å¦åº”è¯¥ç»ˆæ­¢
        """
        # å¼ºåˆ¶ç»ˆæ­¢æ¡ä»¶ï¼šè¾¾åˆ°æœ€å¤§é€‰é¡¹é•¿åº¦
        if self.option_step_count >= self.max_option_length:
            return True

        # ä½¿ç”¨ç»ˆæ­¢ç½‘ç»œåˆ¤æ–­
        with torch.no_grad():
            option_tensor = torch.tensor([option_id], device=state_features.device)
            option_features = self.model.option_encoder(option_tensor)
            termination_prob = self.model.termination_network(state_features, option_features)
            should_terminate = torch.bernoulli(termination_prob).item() > 0.5

        return should_terminate

    def act(self, state_features: torch.Tensor, force_new_option: bool = False) -> Tuple[int, int, Dict]:
        """
        ğŸ”¥ æ™ºèƒ½ä½“å®Œæ•´çš„è¡ŒåŠ¨æµç¨‹
        å®ç°h-DQNçš„åŒå±‚å†³ç­–æœºåˆ¶

        Args:
            state_features: çŠ¶æ€ç‰¹å¾
            force_new_option: æ˜¯å¦å¼ºåˆ¶é€‰æ‹©æ–°é€‰é¡¹

        Returns:
            action_id: é€‰æ‹©çš„åŠ¨ä½œ
            option_id: å½“å‰é€‰é¡¹
            info: é¢å¤–ä¿¡æ¯
        """
        info = {}

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é€‰æ‹©æ–°é€‰é¡¹
        need_new_option = (
                force_new_option or
                self.current_option is None or
                self.should_terminate_option(state_features, self.current_option)
        )

        if need_new_option:
            # ğŸ”¥ å¦‚æœæœ‰ä¹‹å‰çš„é€‰é¡¹ï¼Œè®°å½•å…¶æ‰§è¡Œç»“æœå¹¶æ›´æ–°æˆåŠŸç‡
            if self.current_option is not None and self.option_start_state is not None:
                # æˆåŠŸåˆ¤æ–­é€»è¾‘ï¼ˆå¯æ ¹æ®å…·ä½“ä»»åŠ¡è°ƒæ•´ï¼‰
                # è¿™é‡Œä½¿ç”¨è¾¾åˆ°æœ€å¤§é•¿åº¦ä½œä¸ºæˆåŠŸçš„ç®€å•åˆ¤æ–­
                success = self.option_step_count >= self.max_option_length

                # ğŸ”¥ æ›´æ–°h-DQNæˆåŠŸç‡ç»Ÿè®¡
                self.epsilon_scheduler.update_goal_success(self.current_option, success)
                self.success_tracker.add_result(self.current_option, success)

                info['option_terminated'] = True
                info['option_length'] = self.option_step_count
                info['option_success'] = success
                info['terminated_option'] = self.current_option

            # ğŸ”¥ Meta-controller: é€‰æ‹©æ–°é€‰é¡¹
            self.current_option, option_random = self.select_option(state_features)
            self.option_step_count = 0
            self.option_start_state = state_features.clone()
            self.option_start_step = self.epsilon_scheduler.current_step

            info['new_option_selected'] = True
            info['option_random'] = option_random

        # ğŸ”¥ Controller: åœ¨å½“å‰é€‰é¡¹ä¸‹é€‰æ‹©åŠ¨ä½œ
        action_id, action_random = self.select_action(state_features, self.current_option)
        self.option_step_count += 1

        # ğŸ”¥ æ›´æ–°h-DQNè°ƒåº¦å™¨
        self.epsilon_scheduler.step()

        # æ”¶é›†ä¿¡æ¯
        info.update({
            'current_option': self.current_option,
            'option_step_count': self.option_step_count,
            'action_random': action_random,
            'epsilon_controller': self.epsilon_scheduler.get_epsilon_controller(self.current_option),
            'epsilon_meta': self.epsilon_scheduler.get_epsilon_meta(),  # ğŸ”¥ ç¡®ä¿æ€»æ˜¯å­˜åœ¨
            'option_success_rate': self.success_tracker.get_success_rate(self.current_option),
            'scheduler_stats': self.epsilon_scheduler.get_current_stats()
        })

        return action_id, self.current_option, info


def create_hierarchical_prm_model(config: Dict = None):
    """åˆ›å»ºåˆ†å±‚PRMæ¨¡å‹çš„å·¥å‚å‡½æ•°"""
    if config is None:
        config = {
            'num_options': 4,
            'action_dim': 19,
            'latent_dim': 256,
            'hidden_dim': 512,
            'loss_weights': {
                'img': 1.0,
                'done': 1.0,
                'vector': 1.0,
                'policy': 1.0,
                'value': 0.5,
                'entropy': 0.01,
                'termination': 0.1
            }
        }

    model = HierarchicalPRM(**config)
    return model


def create_enhanced_hierarchical_agent(model_config: Dict = None,
                                       annealing_config: Dict = None) -> HierarchicalAgent:
    """
    ğŸ”¥ åˆ›å»ºé›†æˆh-DQNé€€ç«ç®—æ³•çš„åˆ†å±‚æ™ºèƒ½ä½“

    Args:
        model_config: æ¨¡å‹é…ç½®
        annealing_config: é€€ç«é…ç½®

    Returns:
        agent: åˆ†å±‚æ™ºèƒ½ä½“
    """
    if model_config is None:
        model_config = {
            'num_options': 4,
            'action_dim': 19,
            'latent_dim': 256,
            'hidden_dim': 512
        }

    if annealing_config is None:
        annealing_config = {
            'initial_epsilon_meta': 1.0,  # h-DQNè®ºæ–‡ä¸­Îµ2åˆå§‹å€¼
            'final_epsilon_meta': 0.1,  # h-DQNè®ºæ–‡ä¸­Îµ2æœ€ç»ˆå€¼
            'initial_epsilon_controller': 1.0,  # h-DQNè®ºæ–‡ä¸­Îµ1åˆå§‹å€¼
            'final_epsilon_controller': 0.1,  # h-DQNè®ºæ–‡ä¸­Îµ1æœ€ç»ˆå€¼
            'success_threshold': 0.9,  # h-DQNè®ºæ–‡ä¸­90%æˆåŠŸç‡é˜ˆå€¼
            'anneal_steps_meta': 50000,  # Meta-controlleré€€ç«æ­¥æ•°
            'anneal_steps_controller': 100000  # Controlleré€€ç«æ­¥æ•°
        }

    # åˆ›å»ºæ¨¡å‹
    model = create_hierarchical_prm_model(model_config)

    # ğŸ”¥ åˆ›å»ºh-DQNè°ƒåº¦å™¨å’Œè·Ÿè¸ªå™¨
    epsilon_scheduler = EpsilonAnnealingScheduler(**annealing_config)
    success_tracker = GoalSuccessTracker()

    # ğŸ”¥ åˆ›å»ºåˆ†å±‚æ™ºèƒ½ä½“
    agent = HierarchicalAgent(
        model=model,
        epsilon_scheduler=epsilon_scheduler,
        success_tracker=success_tracker,
        num_options=model_config['num_options'],
        max_option_length=8
    )

    return agent


if __name__ == '__main__':
    print("=== ğŸ”¥ Hierarchical PRM with h-DQN Îµ-greedy Annealing Testing ===")

    # æµ‹è¯•é…ç½®
    BATCH_SIZE = 4
    IMG_C, IMG_H, IMG_W = 3, 210, 160
    NUM_OPTIONS = 4
    ACTION_DIM = 19

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_images = torch.rand(BATCH_SIZE, IMG_C, IMG_H, IMG_W)
    test_options = torch.randint(0, NUM_OPTIONS, (BATCH_SIZE,))
    test_actions = torch.randint(0, ACTION_DIM, (BATCH_SIZE,))
    test_next_images = torch.rand(BATCH_SIZE, IMG_C, IMG_H, IMG_W)
    test_done = torch.randint(0, 2, (BATCH_SIZE,))

    # åˆ›å»ºåŸå§‹æ¨¡å‹æµ‹è¯•
    print("\n=== åŸå§‹åˆ†å±‚PRMæµ‹è¯• ===")
    model = create_hierarchical_prm_model()
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")

    # æµ‹è¯•å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        results = model(test_images, options=test_options, return_all=True)
        print(f"âœ… çŠ¶æ€ç‰¹å¾: {results['state_features'].shape}")
        print(f"âœ… é¢„æµ‹å›¾åƒ: {results['predicted_images'].shape}")
        print(f"âœ… åŠ¨ä½œlogits: {results['action_logits'].shape}")
        print(f"âœ… é€‰é¡¹logits: {results['option_logits'].shape}")

    # ğŸ”¥ åˆ›å»ºå¢å¼ºç‰ˆæ™ºèƒ½ä½“æµ‹è¯•
    print("\n=== ğŸ”¥ h-DQNæ™ºèƒ½ä½“æµ‹è¯• ===")
    agent = create_enhanced_hierarchical_agent()

    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œæµ‹è¯•é€€ç«æœºåˆ¶
    test_state_features = torch.rand(1, 256)

    print(f"\nğŸ§ª æµ‹è¯•h-DQNåŒå±‚Îµ-greedyç­–ç•¥...")
    for step in range(15):
        action_id, option_id, info = agent.act(test_state_features)

        print(f"\nStep {step + 1}:")
        print(f"  åŠ¨ä½œ: {action_id}, é€‰é¡¹: {option_id}")
        print(f"  Îµ2 (meta): {info['epsilon_meta']:.4f}")
        print(f"  Îµ1 (controller): {info['epsilon_controller']:.4f}")
        print(f"  é€‰é¡¹æ­¥æ•°: {info['option_step_count']}")
        print(f"  é€‰é¡¹æˆåŠŸç‡: {info['option_success_rate']:.3f}")

        if info.get('new_option_selected', False):
            print(f"  ğŸ¯ Meta-controlleré€‰æ‹©æ–°é€‰é¡¹! (éšæœº: {info.get('option_random', False)})")
        if info.get('option_terminated', False):
            print(f"  âœ… é€‰é¡¹ç»ˆæ­¢! é•¿åº¦: {info.get('option_length', 0)}, æˆåŠŸ: {info.get('option_success', False)}")

    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    final_stats = agent.epsilon_scheduler.get_current_stats()
    print(f"\nğŸ“Š h-DQNæœ€ç»ˆç»Ÿè®¡:")
    print(f"  è°ƒåº¦å™¨æ­¥æ•°: {final_stats['current_step']}")
    print(f"  å½“å‰Îµ2: {final_stats['epsilon_meta']:.4f}")
    print(f"  ç›®æ ‡æˆåŠŸç‡: {final_stats['goal_success_rates']}")
    print(f"  ç›®æ ‡å°è¯•æ¬¡æ•°: {final_stats['goal_attempts']}")

    # æµ‹è¯•æŸå¤±è®¡ç®—
    print("\n=== åˆ†å±‚æŸå¤±è®¡ç®—æµ‹è¯• ===")
    model.train()
    predictions = model(test_images, options=test_options)
    with torch.no_grad():
        target_state_features, _ = model.encode_state(test_next_images)

    target_data = {
        'target_images': test_next_images,
        'target_done': test_done,
        'target_features': target_state_features,
        'target_actions': test_actions,
        'target_values': torch.randn(BATCH_SIZE),
        'target_termination': torch.randint(0, 2, (BATCH_SIZE,))
    }

    loss, loss_dict = model.compute_hierarchical_loss({}, predictions, target_data)
    print(f"âœ… æ€»æŸå¤±: {loss.item():.6f}")
    for key, value in loss_dict.items():
        print(f"   {key}: {value:.6f}")

    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! é›†æˆh-DQNé€€ç«ç®—æ³•çš„åˆ†å±‚PRMå‡†å¤‡å°±ç»ª!")